
\section{Concentrations of sums of independent random variables}

\subsection{Why concentration inequalities?}

Concentration inequalities quantify the variation of a random variable around its mean, and take the from
\begin{align*}
    \P \{|X - \mu | \geq t\} \leq \text{ something small}
\end{align*}

\begin{prop}{2.1.2}[Tails of the normal distribution]\label{2.1.2}
Let $Z \sim N(0,1)$. For $t>0$
\begin{align*}
    \l( \frac 1 t - \frac1{t^3} \r) \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2} \leq \P \{Z \geq t\} \leq \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
In particular, for $t\geq 1$, the tail of $Z$ has
\begin{align*}
    \P \{Z \geq t\} \leq \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
More loosely, we can say 
\begin{align*}
\P \{Z \geq T\} = \Theta\l(\frac 1 {t e^{t^2/2}}\r) = \tilde{\Theta}\l(\frac 1 {e^{t^2 / 2}}\r)
\end{align*}
\end{prop}

\begin{proof}
For the upper bound, we substitute $x = y+t$ to get
\begin{align*}
    \int^\infty_t\frac 1 {\sqrt{2\pi}}e^{-x^2/2} dx = \int_0^\infty\frac1{\sqrt{2\pi}}e^{-y^2/2}e^{-ty}e^{-t^2/2} dy \leq \frac 1 {\sqrt{2\pi}}e^{-t^2/2}\int_0^\infty e^{-ty} dy = \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
For the lower bound, we make use of the identity
\begin{align*}
    \int^\infty_t e^{-x^2/2}dx \geq \int^\infty_t (1-3x^{-4})e^{-x^2/2}dx = \l(\frac1t - \frac1{t^3}\r)e^{-t^2/2}
\end{align*}
\end{proof}

\begin{eg}{2.1.1}
Consider $S_N = X_1 + ... + X_N$, each $X_i \sim \text{Bernoulli}(1/2)$. We have $\E S_N = N/2, \Var (S_N) = N/4$. From Chebyshev's inequality (Corollary \ref{1.2.5}), we get
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4 \r\} \leq \frac 4 N = O\l(\frac1N\r)
\end{align*}
i.e. the probability of satisfying our concentration requirements goes to 0 linearly. Is this upper bound tight?

We know by the CLT (Theorem \ref{1.3.2}), that our normalized $S_N$ converges in distribution to $N(0,1)$. Then for large $N$, we should see that
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4\r\} = \P \l\{\l|\frac {S_N - \frac N 2}{\sqrt{\frac N 4}}\r| \geq \sqrt{\frac N 4} \r\} \approx \P \l\{ |Z| \geq \sqrt{\frac N 4} \r\} \leq \frac 1 {\sqrt{2 \pi N}}e^{-N/8} = \tilde{O}\l(\frac 1 {e^{N/8}}\r)
\end{align*}
which is exponentially fast (by Proposition \ref{2.1.2}). However this central limit theorem argument can't be made rigourous, since the error in approximating normalized $S_N$ with $Z$ decays too slowly (in fact slower than linearly via Theorem \ref{2.1.3}). It turns out that for these sums, we get light tails much faster than we approximate $N(0,1)$.

\end{eg}

\begin{thm}{2.1.3}[Berry-Esseen CLT]\label{2.1.3}
In the setting of Theorem \ref{1.3.2}, for all $N$
\begin{align*}
    |F_{Z_N}(t) - F_Z(t)| \leq  \frac \rho {\sqrt{N}} \quad\text{for all $t\in\R$}
\end{align*}
where $\rho = \E |X_1 - \mu|^3/\sigma^3$.
\end{thm}
Note that in comparison to Theorem \ref{1.3.2} it additionally requires the third moment $\E X_1^3 < \infty$, and in turn provides a \textit{quantitative} rate for \textit{uniform} convergence in distribution to $N(0,1)$.


\begin{ex}{2.1.4}[Truncated normal distribution]
Let $Z \sim N(0,1)$. Show that for all $t> 0$
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} = t \frac1{\sqrt{2\pi}}e^{-t^2/2} + \P(Z \geq t) \leq \l(t + \frac1t \r) \frac 1 {\sqrt{2\pi}} e^{-t^2/2}
\end{align*}
\end{ex}

\begin{proof}[Answer]
To prove the equality
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} := \int^\infty_t z^2 \frac 1 {\sqrt{2\pi}}e^{-z^2/2} dz = \frac 1 {\sqrt{2\pi}}\l(\l[-ze^{-z^2/2}\r]^\infty_t + \int^\infty_t e^{-z^2/2} dz\r) 
    =t\frac1{\sqrt{2\pi}}e^{-t^2/2} + \P\{Z \geq t\}
\end{align*}
The inequality follows from the tail upper bound from Proposition \ref{2.1.2}.
\end{proof}
\subsection{Hoeffding's inequality}

\begin{defn}{2.2.1}[Rademacher distribution]\label{2.2.1} We say a random variable $X$ has \textbf{Rademacher} distribution if
\begin{align*}
    \P\{X = -1\} = \P\{X = 1\} = \frac 1 2
\end{align*}
\end{defn}

\begin{thm}{2.2.2}[Hoeffding's inequality] \label{2.2.2}
Let $X_1,...,X_N$ be independent Rademacher random variables. Let $a = (a_1,...,a_n) \in \R^N$. For $t\geq 0$
\begin{align*}
\P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp\l( \frac {-t^2}{2\|a\|^2_2}\r)
\end{align*}
\end{thm}
\begin{proof}
WLOG assume $\|a\|_2^2 = 1$. If we prove this version of the theorem, then for any $b = ca \in \R^N$,
\begin{align*}
    \P \l\{\sum_{i=1}^N b_i X_i \geq t\r\} = \P \l\{\sum_{i=1}^N a_i X_i \geq t/c\r\} \leq \exp\l( \frac {-t^2}{2c^2\|a\|^2_2}\r) = \exp\l( \frac {-t^2}{2\|b\|^2_2}\r)
\end{align*}
We apply Markov's inequality to the MGF of $\sum_{i=1}^N a_iX_i$
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} = \P \l\{\exp\l(\lambda\sum_{i=1}^N a_i X_i\r) \geq \exp\l(\lambda t\r)\r\} \leq \frac {\E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r)} {\exp(\lambda t)}
\end{align*}
Examining the numerator of the right side of the inequality
\begin{align*}
    \E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r) &= \prod_{i=1}^N\E \exp(\lambda a_iX_i) &\text{(by independence of $X_i$)}\\
    &= \prod_{i=1}^N \cosh(\lambda a_i) &\text{(by definition of $\E$ of Rademacher RVs)}\\
    &\leq \prod_{i=1}^N \exp(\lambda^2 a_i^2/2) &\text{(by Exercise \ref{2.2.3})}\\
    &= \exp(\lambda^2/2) &\text{(since $\|a\|_2^2$ = 1)}
\end{align*}
To complete the proof we optimize $\lambda$ to minimize the right hand side of the obtained tail bound inequality, $\exp(\lambda^2/2 - \lambda t)$.
Setting $d(\lambda^2/2 - \lambda t)/d\lambda = \lambda - t = 0$ yields the minimum $\lambda = t$. This yields the desired inequality
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp(-t^2/2\|a\|_2^2)
\end{align*} 
\end{proof}

\begin{ex}{2.2.3}[Bounding the hyperbolic cosine]\label{2.2.3}
Show that
\begin{align*} 
    \cosh(x) \leq \exp\l(x^2/2\r) \qquad\text{for all $x\in \R$}
\end{align*}
\end{ex}
\begin{proof}[Answer]
    Recalling that $e^x = \sum_{k=0}^\infty x^k/k!$ for all $x\in \R$, we can compute taylor expansions that converge on $\R$
\begin{align*}
    \cosh(x) &= \frac 1 2 \l(\sum_{k=0}^\infty \frac {x^k} {k!} + \sum_{k=0}^\infty \frac {{(-x)}^k} {k!} \r) = \sum_{k=0}^\infty \frac {x^{2k}} {(2k)!} \\
    e^{x^2/2} &= \sum_{k=0}^\infty \frac {(x^2/2)^k} {k!} = \sum_{k=0}^\infty \frac {x^{2k}}{k! 2^k}
\end{align*}
Note that for $k=0$, the terms match. For $k \geq 1$, 
\begin{align*}
    \frac {x^{2k}} {(2k)!} \le \frac {x^{2k}} {k!2^k} \iff (2k)! \geq k!2^k \iff 2k \cdot ... \cdot k+1 \geq \underbrace{2 \cdot ... \cdot 2}_{\text{$k$ times}}
\end{align*}
where the last statement holds if $k+1 \geq 2 \iff k \geq 1$. Hence for all $x \in \R$, the partial sums of the expansion of $\cosh(x)$ are upper bounded by the partial sums of the expansion of $e^{x^2/2}$, which implies the same for their limits.
\end{proof}

\subsection{Chernoffs's inequality}

