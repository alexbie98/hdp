
\section{Concentrations of sums of independent random variables}

\subsection{Why concentration inequalities?}

Concentration inequalities quantify the variation of a random variable around its mean, and take the from
\begin{align*}
    \P \{|X - \mu | \geq t\} \leq \text{ something small}
\end{align*}

\begin{prop}{2.1.2}[Tails of the normal distribution]\label{2.1.2}
Let $Z \sim N(0,1)$. For $t>0$
\begin{align*}
    \l( \frac 1 t - \frac1{t^3} \r) \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2} \leq \P \{Z \geq t\} \leq \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
In particular, for $t\geq 1$, the tail of $Z$ has
\begin{align*}
    \P \{Z \geq t\} \leq \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
More loosely, we can say 
\begin{align*}
\P \{Z \geq T\} = \Theta\l(\frac 1 {t e^{t^2/2}}\r) = \tilde{\Theta}\l(\frac 1 {e^{t^2 / 2}}\r)
\end{align*}
\end{prop}

\begin{proof}
For the upper bound, we substitute $x = y+t$ to get
\begin{align*}
    \int^\infty_t\frac 1 {\sqrt{2\pi}}e^{-x^2/2} dx = \int_0^\infty\frac1{\sqrt{2\pi}}e^{-y^2/2}e^{-ty}e^{-t^2/2} dy \leq \frac 1 {\sqrt{2\pi}}e^{-t^2/2}\int_0^\infty e^{-ty} dy = \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
For the lower bound, we make use of the identity
\begin{align*}
    \int^\infty_t e^{-x^2/2}dx \geq \int^\infty_t (1-3x^{-4})e^{-x^2/2}dx = \l(\frac1t - \frac1{t^3}\r)e^{-t^2/2}
\end{align*}
\end{proof}

{\begin{eg}{2.1.1}\label{2.1.1}
Consider $S_N = X_1 + ... + X_N$, each $X_i \sim \text{Bernoulli}(1/2)$. We have $\E S_N = N/2, \Var (S_N) = N/4$. From Chebyshev's inequality (Corollary \ref{1.2.5}), we get
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4 \r\} \leq \frac 4 N = O\l(\frac1N\r)
\end{align*}
i.e. the probability of satisfying our concentration requirements goes to 0 linearly. Is this upper bound tight?

We know by the CLT (Theorem \ref{1.3.2}), that our normalized $S_N$ converges in distribution to $N(0,1)$. Then for large $N$, we should see that
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4\r\} = \P \l\{\l|\frac {S_N - \frac N 2}{\sqrt{\frac N 4}}\r| \geq \sqrt{\frac N 4} \r\} \approx \P \l\{ |Z| \geq \sqrt{\frac N 4} \r\} \leq \frac 1 {\sqrt{2 \pi N}}e^{-N/8} = \tilde{O}\l(\frac 1 {e^{N/8}}\r)
\end{align*}
which is exponentially fast (by Proposition \ref{2.1.2}). However this central limit theorem argument can't be made rigourous, since the error in approximating normalized $S_N$ with $Z$ decays too slowly (in fact slower than linearly via Theorem \ref{2.1.3}). It turns out that for these sums, we get light tails much faster than we approximate $N(0,1)$.

\end{eg}

\begin{thm}{2.1.3}[Berry-Esseen CLT]\label{2.1.3}
In the setting of Theorem \ref{1.3.2}, for all $N$
\begin{align*}
    |F_{Z_N}(t) - F_Z(t)| \leq  \frac \rho {\sqrt{N}} \quad\text{for all $t\in\R$}
\end{align*}
where $\rho = \E |X_1 - \mu|^3/\sigma^3$.
\end{thm}
Note that in comparison to Theorem \ref{1.3.2} it additionally requires the third moment $\E X_1^3 < \infty$, and in turn provides a \textit{quantitative} rate for \textit{uniform} convergence in distribution to $N(0,1)$.


\begin{ex}{2.1.4}[Truncated normal distribution]
Let $Z \sim N(0,1)$. Show that for all $t> 0$
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} = t \frac1{\sqrt{2\pi}}e^{-t^2/2} + \P(Z \geq t) \leq \l(t + \frac1t \r) \frac 1 {\sqrt{2\pi}} e^{-t^2/2}
\end{align*}
\end{ex}

\begin{proof}[Answer]
To prove the equality
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} := \int^\infty_t z^2 \frac 1 {\sqrt{2\pi}}e^{-z^2/2} dz = \frac 1 {\sqrt{2\pi}}\l(\l[-ze^{-z^2/2}\r]^\infty_t + \int^\infty_t e^{-z^2/2} dz\r) 
    =t\frac1{\sqrt{2\pi}}e^{-t^2/2} + \P\{Z \geq t\}
\end{align*}
The inequality follows from the tail upper bound from Proposition \ref{2.1.2}.
\end{proof}
\subsection{Hoeffding's inequality}

\begin{defn}{2.2.1}[Rademacher distribution]\label{2.2.1} We say a random variable $X$ has \textbf{Rademacher} distribution if
\begin{align*}
    \P\{X = -1\} = \P\{X = 1\} = \frac 1 2
\end{align*}
\end{defn}

\begin{thm}{2.2.2}[Hoeffding's inequality] \label{2.2.2}
Let $X_1,...,X_N$ be independent Rademacher random variables. Let $a = (a_1,...,a_n) \in \R^N$. For $t\geq 0$
\begin{align*}
\P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp\l( \frac {-t^2}{2\|a\|^2_2}\r)
\end{align*}
\end{thm}

\begin{proof}
WLOG assume $\|a\|_2^2 = 1$. If we prove this version of the theorem, then for any $b = ca \in \R^N$,
\begin{align*}
    \P \l\{\sum_{i=1}^N b_i X_i \geq t\r\} = \P \l\{\sum_{i=1}^N a_i X_i \geq t/c\r\} \leq \exp\l( \frac {-t^2}{2c^2\|a\|^2_2}\r) = \exp\l( \frac {-t^2}{2\|b\|^2_2}\r)
\end{align*}
We apply Markov's inequality to the MGF of $\sum_{i=1}^N a_iX_i$
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} = \P \l\{\exp\l(\lambda\sum_{i=1}^N a_i X_i\r) \geq \exp\l(\lambda t\r)\r\} \leq \frac {\E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r)} {\exp(\lambda t)}
\end{align*}
Examining the numerator of the right side of the inequality
\begin{align*}
    \E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r) &= \prod_{i=1}^N\E \exp(\lambda a_iX_i) &\text{(by independence of $X_i$)}\\
    &= \prod_{i=1}^N \cosh(\lambda a_i) &\text{(by definition of $\E$ of Rademacher RVs)}\\
    &\leq \prod_{i=1}^N \exp(\lambda^2 a_i^2/2) &\text{(by Exercise \ref{2.2.3})}\\
    &= \exp(\lambda^2/2) &\text{(since $\|a\|_2^2$ = 1)}
\end{align*}
To complete the proof we optimize $\lambda$ to minimize the right hand side of the obtained tail bound inequality, $\exp(\lambda^2/2 - \lambda t)$.
Setting $d(\lambda^2/2 - \lambda t)/d\lambda = \lambda - t = 0$ yields the minimum $\lambda = t$. This yields the desired inequality
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp(-t^2/2\|a\|_2^2)
\end{align*} 
\end{proof}

\begin{ex}{2.2.3}[Bounding the hyperbolic cosine]\label{2.2.3}
Show that
\begin{align*} 
    \cosh(x) \leq \exp\l(x^2/2\r) \qquad\text{for all $x\in \R$}
\end{align*}
\end{ex}
\begin{proof}[Answer]
    Recalling that $e^x = \sum_{k=0}^\infty x^k/k!$ for all $x\in \R$, we can compute Taylor expansions that converge on $\R$
\begin{align*}
    \cosh(x) &= \frac 1 2 \l(\sum_{k=0}^\infty \frac {x^k} {k!} + \sum_{k=0}^\infty \frac {{(-x)}^k} {k!} \r) = \sum_{k=0}^\infty \frac {x^{2k}} {(2k)!} \\
    e^{x^2/2} &= \sum_{k=0}^\infty \frac {(x^2/2)^k} {k!} = \sum_{k=0}^\infty \frac {x^{2k}}{k! 2^k}
\end{align*}
Note that for $k=0$, the terms match. For $k \geq 1$, 
\begin{align*}
    \frac {x^{2k}} {(2k)!} \le \frac {x^{2k}} {k!2^k} \iff (2k)! \geq k!2^k \iff 2k \cdot ... \cdot k+1 \geq \underbrace{2 \cdot ... \cdot 2}_{\text{$k$ times}}
\end{align*}
where the last statement holds if $k+1 \geq 2 \iff k \geq 1$. Hence for all $x \in \R$, the partial sums of the expansion of $\cosh(x)$ are upper bounded by the partial sums of the expansion of $e^{x^2/2}$, which implies the same for their limits.
\end{proof}

\begin{rmk}{2.2.4} We can use Hoeffding's to analyze the $N$ coin flips from Example \ref{2.1.1}, achieving the desired (non-asymptotic) exponentially decaying tail probabilities.
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \geq 3N/4\r\} = \P \l\{\sum_{i=1}^N 2X_i -1 \geq N/2\r\} \leq \exp\l(-(N/2)^2/2N\r) = \exp \l(-N/8\r)
\end{align*}
\end{rmk}

\begin{thm}{2.2.6}[Hoeffding's inequality for general bounded RVs] \label{2.2.6}
Let $X_1,...,X_N$ be independent random variables, with each $X_i$'s support $[m_i, M_i]$. For $t>0$
\begin{align*}
    \P \l\{\sum_{i=1}^N (X_i - \E X_i) \geq t\r\} \leq \exp\l( \frac {-2t^2}{\sum_{i=1}^N (M_i - m_i)^2}\r)
    \end{align*}
\end{thm}

\begin{ex}{2.2.7}\label{2.2.7}
Prove Theorem \ref{2.2.6}, possibly with some absolute constant instead of 2 in the tail.
\end{ex}
\begin{proof}[Answer]
We consider $X_i$ with mean 0. For $X_i$ without mean 0, we set $Y_i = X_i - \E X_i$ and proceed in the proof with $Y_i$ which have the same support length as the $X_i$.
The argument follows as in the proof of Theorem \ref{2.2.2} differing only at the part where we obtain a bound for the MGF of the individual $X_i$.

\begin{clm}{}[Hoeffding's lemma]
For a bounded random variable $X \in [m, M]$ with mean 0, we have
\begin{align*}
    \E \exp(\lambda X) \leq \exp (\lambda^2(M-m)^2/2)
\end{align*}
\end{clm}
With the claim we arrive at the inequality
\begin{align*}
    \P \l\{ \sum_{i=1}^N X_i \geq t\r\} \leq \exp\l((\lambda^2/2) \sum_{i=1}^N (M_i -m_i)^2 -\lambda t)\r)
\end{align*}
Optimizing $\lambda$ yields $\lambda = t/\sum_{i=1}^N (M_i - m_i)^2$ and we get
\begin{align*}
\P \l\{ \sum_{i=1}^N X_i \geq t\r\} \leq \exp\l(\frac {-(1/2)t^2}{\sum_{i=1}^N (M_i-m_i)^2}\r)
\end{align*}
Note that the $1/2$ in the tail is looser than the 2 in the theorem statement, which we can get if we prove a tighter version of Hoeffding's lemma (with /8 instead of /2) using Taylor approximations.
For the proof of the claim, we follow an argument by symmetrization presented in the proof of Lemma 5 from these CS229 lecture notes\footnote{\url{http://cs229.stanford.edu/extra-notes/hoeffding.pdf}} by John Duchi.
Consider $X'$ an independent copy of $X$. We have
\begin{align*}
    \E \exp \l(\lambda X\r) = \E_X \exp\l(\lambda (X - \E_{X'} X')\r) \leq \E_X \E_{X'} \exp \l(\lambda (X - X')\r) = \E_X \E_{X',S} \exp \l(\lambda S(X - X')\r) 
\end{align*}
Where the second inequality is by Jensen's. Since $X-X'$ has a symmetric distribution, it has the same distribution as $S (X -X')$, where $S$ is a Rademacher random variable, giving us the last equality.
\begin{align*}
     \E_X \E_{X',S} \exp \l(\lambda S(X - X')\r) &= \E_{X,X'} \cosh \l(\lambda (X - X')\r) &\text{(by definition of $\E_S$ of Rademacher RVs)} \\
     &\leq \E_{X,X'} \exp \l(\lambda^2 (X - X')^2/2\r) &\text{(by Exercise \ref{2.2.3})}\\
     &\leq \exp \l(\lambda^2(M-m)^2/2\r) &\text{(since $|X-X'| \leq |M-m|$)}
\end{align*}
\end{proof}

\begin{ex}{2.2.8}[Boosting randomized algorithms]\label{2.2.8}
Suppose we have a randomized algorithm for a decision problem that is correct with probability $1/2 + \eps$ for some $\eps>0$. Show that running the algorithm $N$ times independently and taking the majority yields the correct answer with probability $\geq 1 -\delta$ for $N\geq (1/2\eps^2) \log \frac 1 \delta$.
\end{ex}
\begin{proof}[Answer]
\end{proof}

\begin{ex}{2.2.9}[Robust mean estimation]\label{2.2.9}
Suppose we want to estimate the mean $\mu$ of a random variable $X$ from $X_1,...,X_N$ copies drawn independently. We want an $\eps$-accurate estimate (falls within $(\eps-\mu, \eps+\mu)$).
\begin{enumerate}[label=(\alph*)]
\item Show a sample size $N = O(\sigma^2/\eps^2)$ is sufficient for an $\eps$-accurate estimate w.p. $\geq 3/4$, where $\sigma^2 = \Var(X)$.
\begin{proof}[Answer]
\end{proof}

\item Show a sample size $N = O(\log(1/\delta)\sigma^2/\eps^2)$ is sufficient for an $\eps$-accurate estimate w.p. $\geq 1-\delta$.
\begin{proof}[Answer]
\end{proof}
\end{enumerate}
\end{ex}


\begin{ex}{2.2.10}[Small ball probabilities]\label{2.2.10}
Let $X_1,...,X_N$ be non-negative independent random variables with continuous distributions. Assume their densities are bounded by 1.
\begin{enumerate}[label=(\alph*)]
\item Show that the MGF of $X_i$ satisfies
\begin{align*}
    \E \exp(-tX_i) \leq \frac 1 t \qquad{\text{for all $t\geq 0$}}
\end{align*}
\begin{proof}[Answer]
\end{proof}

\item Deduce that for any $\eps > 0$ we have
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq \eps N\r\} \leq (e\eps)^N
\end{align*}
\begin{proof}[Answer]
\end{proof}

\end{enumerate}
\end{ex}


\subsection{Chernoffs's inequality}

