
\section{Concentrations of sums of independent random variables}

\subsection{Why concentration inequalities?}

Concentration inequalities quantify the variation of a random variable around its mean, and take the from
\begin{align*}
    \P \{|X - \mu | \geq t\} \leq \text{ something small}
\end{align*}

\begin{prop}{2.1.2}[Tails of the normal distribution]\label{2.1.2}
Let $Z \sim N(0,1)$. For $t>0$
\begin{align*}
    \l( \frac 1 t - \frac1{t^3} \r) \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2} \leq \P \{Z \geq t\} \leq \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
In particular, for $t\geq 1$, the tail of $Z$ has
\begin{align*}
    \P \{Z \geq t\} \leq \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
More loosely, we can say 
\begin{align*}
\P \{Z \geq T\} = \Theta\l(\frac 1 {t e^{t^2/2}}\r) = \tilde{\Theta}\l(\frac 1 {e^{t^2 / 2}}\r)
\end{align*}
\end{prop}

\begin{proof}
For the upper bound, we substitute $x = y+t$ to get
\begin{align*}
    \int^\infty_t\frac 1 {\sqrt{2\pi}}e^{-x^2/2} dx = \int_0^\infty\frac1{\sqrt{2\pi}}e^{-y^2/2}e^{-ty}e^{-t^2/2} dy \leq \frac 1 {\sqrt{2\pi}}e^{-t^2/2}\int_0^\infty e^{-ty} dy = \frac1t \frac 1 {\sqrt {2\pi}}e^{-t^2 / 2}
\end{align*}
For the lower bound, we make use of the identity
\begin{align*}
    \int^\infty_t e^{-x^2/2}dx \geq \int^\infty_t (1-3x^{-4})e^{-x^2/2}dx = \l(\frac1t - \frac1{t^3}\r)e^{-t^2/2}
\end{align*}
\end{proof}

{\begin{eg}{2.1.1}\label{2.1.1}
Consider $S_N = X_1 + ... + X_N$, each $X_i \sim \text{Bernoulli}(1/2)$. We have $\E S_N = N/2, \Var (S_N) = N/4$. From Chebyshev's inequality (Corollary \ref{1.2.5}), we get
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4 \r\} \leq \frac 4 N = O\l(\frac1N\r)
\end{align*}
i.e. the probability of satisfying our concentration requirements goes to 0 linearly. Is this upper bound tight?

We know by the CLT (Theorem \ref{1.3.2}), that our normalized $S_N$ converges in distribution to $N(0,1)$. Then for large $N$, we should see that
\begin{align*}
    \P \l\{ \l|S_N - \frac N 2\r| \geq \frac N 4\r\} = \P \l\{\l|\frac {S_N - \frac N 2}{\sqrt{\frac N 4}}\r| \geq \sqrt{\frac N 4} \r\} \approx \P \l\{ |Z| \geq \sqrt{\frac N 4} \r\} \leq \frac 1 {\sqrt{2 \pi N}}e^{-N/8} = \tilde{O}\l(\frac 1 {e^{N/8}}\r)
\end{align*}
which is exponentially fast (by Proposition \ref{2.1.2}). However this central limit theorem argument can't be made rigourous, since the error in approximating normalized $S_N$ with $Z$ decays too slowly (in fact slower than linearly via Theorem \ref{2.1.3}). It turns out that for these sums, we get light tails much faster than we approximate $N(0,1)$.

\end{eg}

\begin{thm}{2.1.3}[Berry-Esseen CLT]\label{2.1.3}
In the setting of Theorem \ref{1.3.2}, for all $N$
\begin{align*}
    |F_{Z_N}(t) - F_Z(t)| \leq  \frac \rho {\sqrt{N}} \quad\text{for all $t\in\R$}
\end{align*}
where $\rho = \E |X_1 - \mu|^3/\sigma^3$.
\end{thm}
Note that in comparison to Theorem \ref{1.3.2} it additionally requires the third moment $\E X_1^3 < \infty$, and in turn provides a \textit{quantitative} rate for \textit{uniform} convergence in distribution to $N(0,1)$.


\begin{ex}{2.1.4}[Truncated normal distribution]
Let $Z \sim N(0,1)$. Show that for all $t> 0$
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} = t \frac1{\sqrt{2\pi}}e^{-t^2/2} + \P(Z \geq t) \leq \l(t + \frac1t \r) \frac 1 {\sqrt{2\pi}} e^{-t^2/2}
\end{align*}
\end{ex}

\begin{proof}[Answer]
To prove the equality
\begin{align*}
    \E Z^2 \mathbbm{1}_{\{Z\geq t\}} := \int^\infty_t z^2 \frac 1 {\sqrt{2\pi}}e^{-z^2/2} dz = \frac 1 {\sqrt{2\pi}}\l(\l[-ze^{-z^2/2}\r]^\infty_t + \int^\infty_t e^{-z^2/2} dz\r) 
    =t\frac1{\sqrt{2\pi}}e^{-t^2/2} + \P\{Z \geq t\}
\end{align*}
The inequality follows from the tail upper bound from Proposition \ref{2.1.2}.
\end{proof}
\subsection{Hoeffding's inequality}

\begin{defn}{2.2.1}[Rademacher distribution]\label{2.2.1} We say a random variable $X$ has \textbf{Rademacher} distribution if
\begin{align*}
    \P\{X = -1\} = \P\{X = 1\} = \frac 1 2
\end{align*}
\end{defn}

\begin{thm}{2.2.2}[Hoeffding's inequality] \label{2.2.2}
Let $X_1,...,X_N$ be independent Rademacher random variables. Let $a = (a_1,...,a_n) \in \R^N$. For $t\geq 0$
\begin{align*}
\P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp\l( \frac {-t^2}{2\|a\|^2_2}\r)
\end{align*}
\end{thm}

\begin{proof}
WLOG assume $\|a\|_2^2 = 1$. If we prove this version of the theorem, then for any $b = ca \in \R^N$,
\begin{align*}
    \P \l\{\sum_{i=1}^N b_i X_i \geq t\r\} = \P \l\{\sum_{i=1}^N a_i X_i \geq t/c\r\} \leq \exp\l( \frac {-t^2}{2c^2\|a\|^2_2}\r) = \exp\l( \frac {-t^2}{2\|b\|^2_2}\r)
\end{align*}
We apply Markov's inequality to the MGF of $\sum_{i=1}^N a_iX_i$
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} = \P \l\{\exp\l(\lambda\sum_{i=1}^N a_i X_i\r) \geq \exp\l(\lambda t\r)\r\} \leq \frac {\E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r)} {\exp(\lambda t)}
\end{align*}
Examining the numerator of the right side of the inequality
\begin{align*}
    \E \exp\l(\lambda\sum_{i=1}^N a_i X_i \r) &= \prod_{i=1}^N\E \exp(\lambda a_iX_i) &\text{(by independence of $X_i$)}\\
    &= \prod_{i=1}^N \cosh(\lambda a_i) &\text{(by definition of $\E$ of Rademacher RVs)}\\
    &\leq \prod_{i=1}^N \exp(\lambda^2 a_i^2/2) &\text{(by Exercise \ref{2.2.3})}\\
    &= \exp(\lambda^2/2) &\text{(since $\|a\|_2^2$ = 1)}
\end{align*}
To complete the proof we optimize $\lambda$ to minimize the right hand side of the obtained tail bound inequality, $\exp(\lambda^2/2 - \lambda t)$.
Setting $d(\lambda^2/2 - \lambda t)/d\lambda = \lambda - t = 0$ yields the minimum $\lambda = t$. This yields the desired inequality
\begin{align*}
    \P \l\{\sum_{i=1}^N a_i X_i \geq t\r\} \leq \exp(-t^2/2\|a\|_2^2)
\end{align*} 
\end{proof}

\begin{ex}{2.2.3}[Bounding the hyperbolic cosine]\label{2.2.3}
Show that
\begin{align*} 
    \cosh(x) \leq \exp\l(x^2/2\r) \qquad\text{for all $x\in \R$}
\end{align*}
\end{ex}
\begin{proof}[Answer]
    Recalling that $e^x = \sum_{k=0}^\infty x^k/k!$ for all $x\in \R$, we can compute Taylor expansions that converge on $\R$
\begin{align*}
    \cosh(x) &= \frac 1 2 \l(\sum_{k=0}^\infty \frac {x^k} {k!} + \sum_{k=0}^\infty \frac {{(-x)}^k} {k!} \r) = \sum_{k=0}^\infty \frac {x^{2k}} {(2k)!} \\
    e^{x^2/2} &= \sum_{k=0}^\infty \frac {(x^2/2)^k} {k!} = \sum_{k=0}^\infty \frac {x^{2k}}{k! 2^k}
\end{align*}
Note that for $k=0$, the terms match. For $k \geq 1$, 
\begin{align*}
    \frac {x^{2k}} {(2k)!} \le \frac {x^{2k}} {k!2^k} \iff (2k)! \geq k!2^k \iff 2k \cdot ... \cdot k+1 \geq \underbrace{2 \cdot ... \cdot 2}_{\text{$k$ times}}
\end{align*}
where the last statement holds if $k+1 \geq 2 \iff k \geq 1$. Hence for all $x \in \R$, the partial sums of the expansion of $\cosh(x)$ are upper bounded by the partial sums of the expansion of $e^{x^2/2}$, which implies the same for their limits.
\end{proof}

\begin{rmk}{2.2.4} We can use Hoeffding's to analyze the $N$ coin flips from Example \ref{2.1.1}, achieving the desired (non-asymptotic) exponentially decaying tail probabilities.
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \geq 3N/4\r\} = \P \l\{\sum_{i=1}^N 2X_i -1 \geq N/2\r\} \leq \exp\l(-(N/2)^2/2N\r) = \exp \l(-N/8\r)
\end{align*}
\end{rmk}

\begin{thm}{2.2.6}[Hoeffding's inequality for general bounded RVs] \label{2.2.6}
Let $X_1,...,X_N$ be independent random variables, with each $X_i$'s support $[m_i, M_i]$. For $t>0$
\begin{align*}
    \P \l\{\sum_{i=1}^N (X_i - \E X_i) \geq t\r\} \leq \exp\l( \frac {-2t^2}{\sum_{i=1}^N (M_i - m_i)^2}\r)
    \end{align*}
\end{thm}

\begin{ex}{2.2.7}\label{2.2.7}
Prove Theorem \ref{2.2.6}, possibly with some absolute constant instead of 2 in the tail.
\end{ex}
\begin{proof}[Answer]
We consider $X_i$ with mean 0. For $X_i$ without mean 0, we set $Y_i = X_i - \E X_i$ and proceed in the proof with $Y_i$ which have the same support length as the $X_i$.
The argument follows as in the proof of Theorem \ref{2.2.2} differing only at the part where we obtain a bound for the MGF of the individual $X_i$.

\begin{clm}{}[Hoeffding's lemma]
For a bounded random variable $X \in [m, M]$ with mean 0, we have
\begin{align*}
    \E \exp(\lambda X) \leq \exp (\lambda^2(M-m)^2/2)
\end{align*}
\end{clm}
With the claim we arrive at the inequality
\begin{align*}
    \P \l\{ \sum_{i=1}^N X_i \geq t\r\} \leq \exp\l((\lambda^2/2) \sum_{i=1}^N (M_i -m_i)^2 -\lambda t)\r)
\end{align*}
Optimizing $\lambda$ yields $\lambda = t/\sum_{i=1}^N (M_i - m_i)^2$ and we get
\begin{align*}
\P \l\{ \sum_{i=1}^N X_i \geq t\r\} \leq \exp\l(\frac {-(1/2)t^2}{\sum_{i=1}^N (M_i-m_i)^2}\r)
\end{align*}
Note that the $1/2$ in the tail is looser than the 2 in the theorem statement, which we can get if we prove a tighter version of Hoeffding's lemma (with /8 instead of /2) using Taylor approximations.
For the proof of the claim, we follow an argument by symmetrization presented in the proof of Lemma 5 from these CS229 lecture notes\footnote{\url{http://cs229.stanford.edu/extra-notes/hoeffding.pdf}} by John Duchi.
Consider $X'$ an independent copy of $X$. We have
\begin{align*}
    \E \exp \l(\lambda X\r) = \E_X \exp\l(\lambda (X - \E_{X'} X')\r) \leq \E_X \E_{X'} \exp \l(\lambda (X - X')\r) = \E_X \E_{X',S} \exp \l(\lambda S(X - X')\r) 
\end{align*}
Where the second inequality is by Jensen's. Since $X-X'$ has a symmetric distribution, it has the same distribution as $S (X -X')$, where $S$ is a Rademacher random variable, giving us the last equality.
\begin{align*}
     \E_X \E_{X',S} \exp \l(\lambda S(X - X')\r) &= \E_{X,X'} \cosh \l(\lambda (X - X')\r) &\text{(by definition of $\E_S$ of Rademacher RVs)} \\
     &\leq \E_{X,X'} \exp \l(\lambda^2 (X - X')^2/2\r) &\text{(by Exercise \ref{2.2.3})}\\
     &\leq \exp \l(\lambda^2(M-m)^2/2\r) &\text{(since $|X-X'| \leq |M-m|$)}
\end{align*}
\end{proof}

\begin{ex}{2.2.8}[Boosting randomized algorithms]\label{2.2.8}
Suppose we have a randomized algorithm for a decision problem that is correct with probability $1/2 + \eps$ for some $\eps>0$. Show that running the algorithm $N$ times independently and taking the majority yields the correct answer with probability $\geq 1 -\delta$ for $N\geq (1/2\eps^2) \log (1/ \delta)$.
\end{ex}
\begin{proof}[Answer]
Suppose the input to our algorithm $A$ is a YES instance, and define the random variable $X_i$
\begin{align*}
    X_i =\begin{cases}
        \phantom{-}1 &\text{if $i$th run of $A$ outputs YES w.p. } 1/2 + \eps \\
        -1 &\text{if $i$th run of $A$ outputs NO\phantom{ } w.p. } 1/2 - \eps
    \end{cases}
\end{align*}
Then we have
\begin{align*}
    \P \{\text{Majority}(X_1,...,X_N) = -1 \} = \P \l\{\sum_{i=1}^N X_i \leq 0\r\} = \P \l\{\sum_{i=1}^N (X_i - 2\eps) \leq -2N\eps\r\} \leq \exp\l(-2N\eps^2\r)
\end{align*}
where the last inequality is obtained by Hoeffding's inequality (Theorem \ref{2.2.6}) applied to the bounded random variables $-X_i$'s. Finally we note that 
\begin{align*}
    N \geq \frac 1 {2\eps^2}\log \frac 1 \delta \iff 2N\eps^2 \geq \log \frac 1 \delta \iff \exp\l(-2N\eps^2\r) \leq \delta
\end{align*}
Therefore our algorithm is correct on YES instances with probability $\geq 1-\delta$. We can use the same argument to conclude the same on NO instances, completing the proof.
\end{proof}

\begin{ex}{2.2.9}[Robust mean estimation]\label{2.2.9}
Suppose we want to estimate the mean $\mu$ of a random variable $X$ from $X_1,...,X_N$ copies drawn independently. We want an $\eps$-accurate estimate (falls within $(\eps-\mu, \eps+\mu)$).
\begin{enumerate}[label=(\alph*)]
\item Show a sample size $N = O(\sigma^2/\eps^2)$ is sufficient for an $\eps$-accurate estimate w.p. $\geq 3/4$, where $\sigma^2 = \Var(X)$.
\begin{proof}[Answer]
Note that we can't directly apply Hoeffding's since we don't know if $X$ is bounded. However we can apply Chebyshev's (Corollary \ref{1.2.5})
\begin{align*}
    \P \l\{ \l|\frac1N\sum_{i=1}^N X_i - \mu \r| \geq \eps\r\} \leq \Var\l( \frac1N\sum_{i=1}^N X_i\r)/\eps^2 = \sigma^2/N\eps^2
\end{align*}
and $\sigma^2/N\eps^2 \leq 1/4 \iff N \geq 4(\sigma^2/\eps^2)$, giving our $\geq 3/4$ success probability with $N = O(\sigma^2/\eps^2)$ samples.
\end{proof}

\item Show a sample size $N = O(\log(1/\delta)\sigma^2/\eps^2)$ is sufficient for an $\eps$-accurate estimate w.p. $\geq 1-\delta$.
\begin{proof}[Answer]
Note that plugging in $\delta$ into Chebyshev's in (a) would give us a $N = O(\sigma^2/\eps^2\delta)$ sample requirement to achieve the desired success probability, which has much worse dependence on $\delta$. What we can do instead is boost our weak estimator by running it $k$ times and producing the median. Let our weak estimates be $\hat{\mu}_i$ obtained using $N$ samples each for $1 \leq i \leq k$ (for $k$ odd). Note that $\text{Median}(\hat{\mu}_1,...,\hat{\mu}_k)$ is outside of $(\mu -\eps, \mu +\eps)$ iff there are either $\geq (k+1)/2$ estimates $> \mu +\eps$ or $\geq (k+1)/2$ estimates $< \mu -\eps$. In either case, we have that our weak estimator failed $\geq (k+1)/2$ times. Letting $F_i = 1 \text{ when $\hat{\mu}_i \in (\mu-\eps, \mu+\eps)$}$ and 0 otherwise, we get
\begin{align*}
    \P \l\{\text{Median}(\hat{\mu}_1,...,\hat{\mu}_k) \text{ is not $\eps$-accurate}\r\} &\leq \P \l\{\sum_{i=1}^k F_i \geq \frac {k+1}2\r\} \\
    &= \P \l\{\sum_{i=1}^k (F_i - \E F_1) \geq \frac {k+1}2 - k\E F_1\r\} \\
    &\leq \P \l\{\sum_{i=1}^k (F_i - \E F_1) \geq \frac {k+1}2 - \frac k 4\r\} &\text{(by $\E F_1 \leq 1/4$)} \\
    &\leq \exp\l(-2\l(\frac {k+2} 4\r)^2/k\r) &\text{(Hoeffding's, Thm. \ref{2.2.6})} \\
    &\leq \exp\l(-k/8\r) &\text{(since $(k+2)/4 \geq k/4$)}
\end{align*}
Taking odd $k \geq 8\log (1/\delta)$ bounds the error probability to $\leq \delta$ and uses $kN \leq  (8\log (1/\delta) +2) 4(\sigma^2/\eps^2) = O(\log(1/\delta)\sigma^2/\eps^2)$ samples.

\end{proof}
\end{enumerate}
\end{ex}


\begin{ex}{2.2.10}[Small ball probabilities]\label{2.2.10}
Let $X_1,...,X_N$ be non-negative independent random variables with continuous distributions. Assume their densities are bounded by 1.
\begin{enumerate}[label=(\alph*)]
\item Show that the MGF of $X_i$ satisfies
\begin{align*}
    \E \exp(-tX_i) \leq \frac 1 t \qquad{\text{for all $t > 0$}}
\end{align*}
\begin{proof}[Answer]
For non-negative $X_i$ with bounded density function $0 \leq p(x) \leq 1$
\begin{align*}
    \E \exp\l(-tX_i\r) = \int^\infty_0  p(x) e^{-tx} dx \leq \int_0^\infty e^{-tx} dx = \l[-\frac 1 t e^{-tx}\r]^\infty_0 = \frac 1 t
\end{align*}
\end{proof}

\item Deduce that for any $\eps > 0$ we have
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq \eps N\r\} \leq (e\eps)^N
\end{align*}
(This result essentially says that for ``sufficiently distributed'' (at least, with support of measure $\geq 1$) non-negative RVs, their sum is unlikely to be bounded by any constant fraction of the number of trials.)
\begin{proof}[Answer]
Consider the MGF for $-\sum_{i=1}^N X_i$, evalutated at $\lambda = 1/\eps$
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq \eps N\r\} &= \P \l\{\sum_{i=1}^N -X_i \geq -\eps N\r\} \\
    &\leq \P \l\{\exp\l(-\frac 1 {\eps}\sum_{i=1}^N X_i\r) \leq e^ {- N }\r\} \\
    &\leq\l(\prod_{i=1}^N \E \exp\l(-\frac1{\eps} X_i\r)\r) /e^{-N} &\text{(by Markov's (Prop. \ref{1.2.4}) and indep.)} \\
    &\leq (e\eps)^N &\text{(applying part (a) with $t=1/\eps$)}
\end{align*}
Note that this choice of $\lambda$ minimizes the right hand side of the expression, which can be verified by taking derivatives w.r.t $\lambda$.
\end{proof}
\end{enumerate}
\end{ex}


\subsection{Chernoff's inequality}

In the case where $X_i \sim \text{Bernoulli}(p_i)$ we can get tighter bounds than Hoeffding's.

\begin{thm}{2.3.1}[Chernoff's inequality]\label{2.3.1}
Let $X_1,...,X_N$ be independent Bernoulli random variables with parameters $p_i$ for $1 \leq i \leq N$. Denote $\mu = \E\sum_{i=1}^N X_i = \sum_{i=1}^N p_i$. For $t>\mu$
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \geq t \r\} \leq e^{-\mu}\l(\frac {e \mu} {t}\r)^t
\end{align*}
\end{thm}

\begin{ex}{2.3.2}[Chernoff's inequality, lower tail]\label{2.3.2}
In the setting of Theorem \ref{2.3.1}, show that for $t < \mu$
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq t \r\} \leq e^{-\mu}\l(\frac {e \mu} {t}\r)^t
\end{align*}
\end{ex}
\begin{proof}[Answer]
This result can be obtained by modifying the proof of Theorem \ref{2.3.1}.
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq t \r\} &= \P \l\{\exp\l(-\lambda\sum_{i=1}^N X_i\r) \geq e^{-\lambda t} \r\} &\text{(for any $\lambda\geq 0$)} \\
    &\leq \E \exp\l(-\lambda\sum_{i=1}^N X_i\r) \cdot e^{\lambda t} &\text{(by Markov's (Prop. \ref{1.2.4}))} \\
    &= e^{\lambda t} \cdot \prod_{i=1}^N \E \exp\l( -\lambda X_i\r) &\text{(by independence)}
\end{align*}
For each term, we have
\begin{align*}
    \E \exp(-\lambda X_i) = p_i e^{-\lambda} + (1-p_i) = 1 + p_i\l(e^{-\lambda} -1\r) \leq \exp \l(p_i\l(e^{-\lambda} - 1\r)\r)
\end{align*}
which yields
\begin{align*}
    \P \l\{\sum_{i=1}^N X_i \leq t \r\} &\leq \exp \l( \lambda t + \mu(e^{-\lambda} - 1)\r) &\text{(since $\mu= \sum_{i=1}^N p_i$)} \\
    &= \exp \l(t \ln \frac \mu t + t - \mu\r) &\text{(plugging in $\lambda = \ln \frac \mu t > 0$ by $\mu>t$)} \\
    &= e^{-\mu}\l(\frac {e\mu} t\r)^t
\end{align*}
\end{proof}

\begin{ex}{2.3.3}[Poisson tails]\label{2.3.3}
Let $X \sim \text{Poisson}(\lambda)$. Show that for $t > \lambda$
\begin{align*}
    \P \l\{X \geq t \r\} \leq e^{-\lambda} \l( \frac {e\lambda} {t}\r)^t
\end{align*}
\begin{proof}[Answer] Consider the MGF of $X$, and apply Markov's inequality (Prop. \ref{1.2.4}). For all $c \geq 0$, we have
\begin{align*}
    \P \l\{ X \geq t\r\} \leq \P \l\{e^{cX} \leq e^{ct}\r\} \leq \E e^{cX} \cdot e^{-ct} = e^{-ct}\cdot\sum_{k=0}^\infty e^{-\lambda}\frac{\lambda^k}{k!}e^{ck} = e^{-\lambda} \cdot e^{-ct} \sum_{k=0}^\infty\frac{(\lambda e^c)^k}{k!}
\end{align*}
Taking $c = \ln (t/\lambda) > 0$, since by assumption $t > \lambda$
\begin{align*}
    \P \{ X \geq t\} \leq e^{-\lambda}\cdot\l(\frac {\lambda} t\r)^t \cdot e^{\lambda (t/\lambda)} = e^{-\lambda}\l(\frac {e\lambda} t\r)^t
\end{align*}

\end{proof}
\end{ex}

\begin{rmk}{2.3.4}[Poisson tails]\label{2.3.4}
Applying Stirling's formula $k! \approx \sqrt{2\pi k}(k/e)^k$ to the Poisson density yields
\begin{align*}
    \P \{X = k\} \approx \frac 1 {\sqrt{2 \pi k}} e^{-\lambda} \l(\frac{e \lambda}k \r)^k
\end{align*}
So the tail bound of all the mass $\geq k$ from Exercise \ref{2.3.3} is only a logarithmic factor (w.r.t. main term) larger than the mass assigned to only $k$.
\end{rmk}

\begin{ex}{2.3.5}[Chernoff's inequality, small deviations]\label{2.3.5}
In the setting of Theorem \ref{2.3.1}, for $0 < \delta < 1$
\begin{align*}
    \P \l\{\l| \sum_{i=1}^N X_i - \mu\r| \geq \delta\mu\r\} \leq 2\exp\l(-c\mu\delta^2\r)
\end{align*}
where $c$ is some absolute constant. Compared to the Hoeffding bound that has $-c\mu^2\delta^2/N$ in its exponent, we have $-c\mu\delta^2$, a $N/\mu \geq 1$ multiplier which speeds up convergence especially for small $\mu = \sum_{i=1}^N p_i$. 
\end{ex}

\begin{proof}[Answer]
    To bound the upper tail, since $(1+\delta) \mu > \mu$ we can apply Theorem \ref{2.3.1}
\begin{align*}
    \P \l\{\sum_{i=0}^N X_i - \mu \geq \delta \mu\r\} = 
    \P \l\{\sum_{i=0}^N X_i \geq (1+\delta) \mu\r\} \leq e^{-\mu}\l(\frac {e\mu} {(1+\delta)\mu}\r)^{(1+\delta)\mu} = \exp\l(\delta\mu - \ln (1+\delta)(1+\delta)\mu\r)
\end{align*}
If we can find $c>0$ such that $\delta - \ln(1+\delta)(1+\delta) \leq -c\delta^2$, equivalently that $\ln(1+\delta)(1+\delta) - \delta \geq c\delta^2$ for all $0 < \delta < 1$, then by the previous string of inequalities we get
\begin{align*}
    \P \l\{\sum_{i=0}^N X_i - \mu \geq \delta \mu\r\} \leq \exp\l(-c\delta^2\mu\r)
\end{align*}
Using the Maclaourin series for $\ln(1+x)$, we have that for $x \geq 0$
\begin{align*}
    \ln (1 + x) \geq x - \frac {x^2} 2 + \frac {x^3} 3 - \frac {x^4}4 
\end{align*}
Plugging this in, we get
\begin{align*}
    \ln(1+\delta)(1+\delta) - \delta &\geq (\delta - \frac {\delta^2} 2 + \frac {\delta^3} 3 - \frac {\delta^4}4 )(1+\delta) - \delta \\
    &=\frac{\delta^2} 2 - \frac{\delta^3} 6 + \frac {\delta^4} {12} - \frac{\delta^5} 4 \\
    &\geq \frac {\delta^2} {12} &\text{(since $-\delta^3, -\delta^4 \geq -\delta^2$ on $\delta \in [0,1]$)}
\end{align*}
So choosing $c = 1/12$ suffices to prove the result for the upper tail. For the lower tail, we are trying to bound
\begin{align*}
    \P \l\{\sum_{i=0}^N X_i - \mu \leq -\delta \mu\r\} = 
    \P \l\{\sum_{i=0}^N X_i \leq (1-\delta) \mu\r\} \leq \exp \l(-c\delta^2\mu\r)
\end{align*} 
Note that since $(1-\delta)\mu < \mu$, we can apply Exercise \ref{2.3.2}, and proceed exactly as in the upper tail calculation to arrive at the same bound. The event of observing absolute deviation from the mean $\geq \delta\mu$ is precisely the disjoint union of these two events, so we add the probabilities to get the desired result.

\end{proof}

\begin{ex}{2.3.6}[Poisson distribution near the mean]\label{2.3.6}
Let $X \sim \text{Poisson}(\lambda)$. Show that for $0 < t \leq \lambda$
\begin{align*}
    \P \l\{ |X - \lambda| \geq  t \r\} \leq 2\exp\l(- \frac {ct^2} \lambda \r)
\end{align*}
for some absolute constant $c$. Note that this, combined with Exercise \ref{2.3.3} tell us that we have gaussian tails near the mean, but further away they are heavier.
\end{ex}
\begin{proof}[Answer]
The proof proceeds exactly as in Exercise \ref{2.3.5}. We'll bound the upper tail using the result of Exercise \ref{2.3.3}, the lower tail can be bounded by a variation of the result of Exercise \ref{2.3.3} adapted for the lower tail (following how Exercise \ref{2.3.2} is proven). Let $0 < t \leq \lambda$
\begin{align*}
    \P \{X - \lambda \geq t\} =  \P \l\{X \geq \lambda \l(1 +  t/\lambda\r)\r\} \leq e^{-\lambda}\l(\frac {e\lambda} {\lambda(1 + t/\lambda)}\r)^{\lambda(1+t/\lambda)} = \exp \l(\lambda\delta - \ln(1+\delta)(1+\delta)\lambda\r)
\end{align*}
taking $\delta = t/\lambda \in (0,1]$. Using the Maclaurin series for $\ln(1+x)$, we have that for $\delta \in (0,1]$
\begin{align*}
    \exp(\delta\lambda - \ln(1 + \delta)(1+\delta)\lambda) \leq \exp \l(\frac {-\delta^2\lambda} {12}\r) =  \exp\l(\frac {-ct^2} {\lambda}\r)
\end{align*}
\end{proof}
\begin{ex}{2.3.8}[Normal approximation to Poisson]\label{2.3.8}
Let $X \sim \text{Poisson}(\lambda)$. Show that as $\lambda\to\infty$
\begin{align*}
    \frac {X - \lambda} {\sqrt{\lambda}} \to N(0,1) \quad\text{in distribution}
\end{align*}
\end{ex}
\begin{proof}[Answer]
The result follows from applying the Berry-Esseen CLT (Theorem \ref{2.1.3}) with $N=1$.\footnote{The hint suggests to use the fact that $X = \sum_{i=1}^N X_i$ where $X_i$ are independent Poisson RVs, and then applying the regular CLT. I couldn't find a proof this way except for the special case where $\{\lambda_n\} = n \in \N$ for $n \to \infty$.} In the setting of the theorem, $\rho = \E | X - \mu|^3/\sigma^3 = \lambda/\lambda^{3/2}=1/\sqrt{\lambda}$ (the mean, variance, and third centered moment of $X\sim \text{Poisson}(\lambda)$ are all $\lambda$). Therefore
\begin{align*}
    \l|\P\l\{\frac {X-\lambda}{\sqrt{\lambda}} \leq t\r\} - \Phi(t)\r| \leq \frac \rho {\sqrt{N}} = \frac 1 {\sqrt{\lambda}} \to 0 \text{ as $\lambda\to \infty$, for all $t\in \R$ simultaneously}
\end{align*}
(Note that the result we proved here is stronger than convergence in distribution, which only requires pointwise convergence of the CDFs. Here we have that the CDFs converge \textit{uniformly} on $\R$.)
\end{proof}