\section{Preliminaries on random variables}
\subsection{Basic quantities}

The \textbf{expection} of a random variable $X$ is denoted as $\E X$, and \textbf{variance} is denoted as $\Var(X) = \E (X - \E X)^2$. 
(We note that the expectation operator $\E$ can be directly defined as the Lebesgue integral of the random variable (measurable function) $X: \Omega \rightarrow \R$ in the probability space $(\Omega, \M, \P)$.

The \textbf{moment generating function} of $X$ is given by
\begin{align*}
    M_X(t) = \E e^{tX} \qquad\qquad\text{for all $t\in \R$}
\end{align*}
The \textbf{p-th moment} of $X$ is given by $\E X^p$. We also let $\|X\|_p = ( \E X^p )^{\frac{1}{p}}$ denote the \textbf{p-norm} of $X$. For $p=\infty$, we have
\begin{align}
    \|X\|_\infty = \operatorname{ess}\operatorname{sup}X \nonumber
\end{align}
recalling that the \textbf{essential supremum} of a function $f$ is the "smallest value $\gamma$ such that $\{\omega \in \Omega: |f(\omega)| > \gamma \}$ has measure 0".

From this, we can define the \textbf{$\boldsymbol{L^p}$ spaces}%
\footnote{A technical note is that the objects of $L_p$ are actually equivalence classes of functions $[X]$ with equality almost everywhere, otherwise $\|\Cdot\|_p$ is only a semi-norm.},
given a probability space $(\Omega, \M, \P)$
\begin{align*}
    L^p = \{X : \|X\|_p < \infty\}
\end{align*}
Results from measure and integration theory tell us that the $(L^p, \|\Cdot\|_p)$ are complete. In the case of $L^2$, we have that with the inner product
\begin{align*}
    \langle X,Y \rangle &= \int_\Omega{XY(\omega)\mu(\omega)} \\
    &= \E XY
\end{align*}
$(L^2, \langle \Cdot,\Cdot \rangle)$ is a Hilbert space. In this case we can express the \textbf{standard deviation} of X as $\sqrt{\operatorname{Var}(X)} = \|X - \E X\|_2$, and the \textbf{covariance} of random variable $X$ and $Y$ as
\begin{align*}
    \operatorname{Cov}(X,Y) = \E (X - \E X)(Y - \E Y) = \langle X - \E X, Y - \E Y\rangle )
\end{align*}

In this setting, considering random variables as vectors in $L^2$, the covariance between $X$ and $Y$ can be interpreted as the \textit{alignment} between the vectors $X - \E X$ and $Y - \E Y$.

\subsection{Some classical inequalities}

We say $f : \R \to \R$ is \textbf{convex} if
\begin{align*}
    f((1-t)x + ty) \leq (1-t)f(x) + tf(y) &\qquad\qquad\text{for all $x,y\in\R$ and $t\in[0,1]$}
\end{align*}
\textbf{Jensen's inequality} states that for any random variable $X$ and a convex function $f$, we get 
\begin{align*}
    f(\E X) \leq \E(f(X))
\end{align*}
A corollary of Jensen's inequality implies that\footnote{For $q < \infty$, the result follows by applying Jensen's inequality for $f(x) = x^{\frac q p}$. Otherwise, $\|X\|_\infty = \gamma = (\E \gamma^{p})^{\frac1p} = \|\gamma\|_p \geq \|X\|_p$.}
\begin{align*}
    \| X\|_p \leq \|X\|_q \qquad\qquad\text{for all $1\leq p \leq q\leq \infty$}
\end{align*}

\textbf{Minkowski's inequality} asserts that the triangle inequality holds for the $L_p$ spaces
\begin{align*}
\|X +  Y\|_p \leq \|X\|_p + \|Y\|_p \qquad\qquad\text{for all $X,Y \in L^p$}
\end{align*}

In $L^2$, we have the \textbf{Cauchy-Schwarz inequality}, which states that $|\E XY| \leq \E |XY| \leq \|X\|_2 \|Y\|_2$. \textbf{Holder's inequality} additionally asserts that for $1/p + 1/q =1$
\begin{align*}
    |\E X Y| \leq \|XY\|_1 \leq \|X\|_p\|Y\|_q
\end{align*}
which also holds for $p=1, q=\infty$.

The \textbf{cumulative distribution function} of X is defined as
\begin{align*}
    F_X(t) = \P\{X \leq t\} = \P(X^{-1}(-\infty,t]) \qquad\text{for all $t\in\R$}
\end{align*}
and we refer to $\P\{X > t\} = 1 - F_X(t)$ as the \textbf{tail} of X.

% lemma 1.2.1 - integral identity ---------------------

\begin{lemma}{1.2.1}[Integral identity]\label{1.2.1}
Let $X\geq0$ be a random variable. Then 
\begin{align*}
    \E X = \int^\infty_0\P\{X>t\}dt
\end{align*}
with left side $=\infty$ iff right side $=\infty$.
\end{lemma}

\begin{ex}{1.2.2}[Generalization of integral identity]\label{1.2.2}
Show that Lemma  can be extended to be valid for any $X$
\begin{align*}
    \E X = \int^\infty_0 \P\{X >t\} dt - \int^0_{-\infty} \P\{X < t\} dt
\end{align*}
\end{ex}
\begin{proof}[Answer]
For not necessary non-negative $X$, $\E X := \E X^+ - \E X^-$ when they exist and are both $< \infty$, where
\begin{align*}
    X^+ = \begin{cases}
            X &\text{if } X\geq0 \\
            0 &\text{otherwise}
    \end{cases}\qquad
    X^- = \begin{cases}
            -X &\text{if } X\leq0 \\
            0 &\text{otherwise}
    \end{cases}
\end{align*}
Applying Lemma \ref{1.2.1} to the terms yields the result. For the second term
\begin{align*}
    \E X^- = \int^\infty_0 \P\{X^- > t\} dt = \int^\infty_0\P\{X < -t\} dt = \int^0_{-\infty}\P\{X < t\} dt
\end{align*}
\end{proof}

\begin{ex}{1.2.3}[$p$-th moment via the tail]\label{1.2.3}
Let $X$ be a random variable and $0 < p < \infty$. Show that
\begin{align*}
    \E |X|^p = \int^\infty_0 pt^{p-1}\P\{|X|>t\} dt
\end{align*}
whenever the right side is $<\infty$.
\end{ex}
\begin{proof}[Answer]
On the right side, substitute $u = t^p$, so $du = pt^{p-1} dt$ and
\begin{align*}
    \int^\infty_0 pt^{p-1}\P\{|X|>t\} dt = \int^\infty_0 \P\{|X|>u^{\frac1p}\} du = \int^\infty_0 \P\{|X|^p>u\} du = \E |X|^p
\end{align*}
where the last equality comes from applying Lemma \ref{1.2.1} to the random variable $|X|^p \geq 0$.
\end{proof}

% prop 1.2.4+ - markov's & chebyshev's inequalities ---------------------

\begin{prop}{1.2.4}[Markov's inequality]\label{1.2.4}
Let $X \geq 0$ with $\E X < \infty$. Then for $t > 0$
\begin{align*}
    \P \{ X \geq t\} \leq \frac{\E X} t
\end{align*}
\end{prop}

\begin{proof}
Fix $t>0$. Applying Lemma \ref{1.2.1}
\begin{align*}
    \E X = \int_0^\infty \P\{X \geq x\} dx \geq \int_0^t \P\{X \geq x\} dx \geq \int_0^t \P\{X \geq t\} dx = t\cdot\P\{X \geq t\}
\end{align*}
\end{proof}

\begin{cor}{1.2.5}[Chebyshev's inequality]\label{1.2.5}
Let $X$ have $\E X <\infty$ and $\Var(X) <\infty$. Then for $t > 0$
\begin{align*}
    \P \{ |X -\E X| > t\} \leq \frac{\Var(X)} {t^2}
\end{align*}
\end{cor}

\begin{ex}{1.2.6}\label{1.2.6}
Give a proof of Chebyshev's inequality using Markov's inequality.
\end{ex}
\begin{proof}[Answer]
The random variable $|X - \E X|^2$ is well-defined (by $\E X < \infty$), non-negative, with finite expectation. 
Applying Markov's inequality with $t^2>0$ yields
\begin{align*}
    \P \{|X -\E X| \geq t\} =
    \P \{|X -\E X|^2 \geq t^2\} \leq \frac {\Var(X)}{t^2}
\end{align*}
\end{proof}

\subsection{Limits theorems}

For independent and identically distributed variables $X_1,...,X_N$, the sample mean $\frac1N \sum_{i=1}^N X_i$ has 
\begin{align*}
    \Var(\frac1N\sum_{i=1}^N X_i) = \frac {\Var(X_1)} N \to 0 \text{ as $N\to \infty$}
\end{align*}
so we should expect it to concentrate around the true mean.

\begin{thm}{1.3.1}[Strong law of large numbers]\label{1.3.1}
Let $X_1, X_2,...$ be a sequence of i.i.d. random variables with $\E X_1 < \infty$. Then the averaged partial sums
\begin{align*}
    \frac {S_N} {N} = \frac 1 N \sum_{i=1}^N X_i \to \E X_1 \quad\text{almost surely}
\end{align*}

where random variables $(Y_N)_{N=1}^\infty$ are said to \textbf{converge almost surely} to a random variable $Y$ if there exists measurable $Z \in \M$ with $\P (Z) = 0$ and
\begin{align*}
    \lim_{N\to\infty}Y_N(\omega) = Y(\omega) \quad\text{for every $\omega \in \Omega \setminus Z$}
\end{align*}
\end{thm}

\begin{thm}{1.3.2}[Lindeberg-LÃ©vy CLT]\label{1.3.2}
Let $X_1, X_2, ....$ be a sequence of i.i.d. random variables with $\E X_1 = \mu, \Var(X_1) = \sigma^2 < \infty$. Then the normalized partial sums
\begin{align*}
Z_N =\frac {S_N - \E S_N} {\sqrt{\Var(S_N)}}= \frac{\sum_{i=1}^N X_i - N\mu}{\sigma\sqrt{N}} \to N(0,1) \quad\text{in distribution}
\end{align*}

where real random variables $(Y_N)_{N=1}^\infty$ are said to \textbf{converge in distribution} to a random variable $Y$ if their CDF's $F_{Y_N}(t) := \P \{Y_N \leq t\}$, $F_Y(t):= \P \{Y \leq t\}$ have
\begin{align*}
    \lim_{N\to\infty}F_{Y_N}(t) = F_Y(t) \quad\text{for all $t\in \R$}
\end{align*}
\end{thm}

\begin{ex}{1.3.3}\label{1.3.3} Let $X_1, X_2, ...$ be a sequence of i.i.d. random variables with $\mu, \sigma^2 < \infty$. Show that
\begin{align*}
    \E \l|\frac 1 N \sum_{i=1}^N X_i - \mu\r| = O(\frac 1 {\sqrt N})
\end{align*}
\end{ex}
\begin{proof}[Answer]
Considering the convex function $\phi(x) = x^2$, we can apply Jensen's to get
\begin{align*}
    \l(\E \l|\frac 1 N \sum_{i=1}^N X_i - \mu\r|\r)^2 \leq \Var\l(\frac 1 N \sum_{i=1}^N X_i\r) = \frac {\sigma^2} N
\end{align*}

taking the square root of both sides yields the result.
\end{proof}

\begin{thm}{1.3.4}[Poisson limit theorem]
Consider a sequence of $N$-tuples of independent random variables with entries $X_{Ni}$ for $1 \leq i \leq N$ with $X_{Ni} \sim \text{Bernoulli}(p_{Ni})$. Let $S_N = \sum_{i=1}^N X_{Ni}$, and suppose that as $N\to\infty$
\begin{align*}
    \max_{1 \leq i \leq N} p_{Ni} \to 0 \quad\text{and}\quad \E S_N = \sum_{i=1}^N p_{Ni}\to \lambda
\end{align*}

Then $S_N \to \text{Poisson}(\lambda)$ in distribution, i.e. the CDF $F_{S_N}(t) = \P \{S_N \leq t\}$ has
\begin{align*}
    \lim_{N\to\infty} F_{S_N}(t) = \sum_{k=1}^{\lfloor t \rfloor} e^{-\lambda} \frac {\lambda^k} {k!}
\end{align*}
\end{thm}